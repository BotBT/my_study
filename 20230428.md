# 4월 28일
## Python

### practice5
- 1번.
1. 거꾸로 배열해도 같은 단어 또는 문장이 되는
2.회문(pralindrome)인지 판별하는 함수를 정의하시오
3. 함수에 문자열을 입력받고 회문하면 True
4. 아니면 False를 반환하도록 정의하세요.
5. 함수 이름 : is_palindrome
6. 반환 값 : True 또는 False

```python
# 예제 1.
word = input('단어를 입력하세요:')

is_palindrome = True                 # 회문 판별값을 저장할 변수, 초깃값은 True
for i in range(len(word) // 2):      # 0부터 문자열 길이의 절반만큼 반복
    if word[i] != word[-1 - i]:      # 왼쪽 문자와 오른쪽 문자를 비교하여 문자가 다르면
        is_palindrome = False        # 회문이 아님
        break
 
print(is_palindrome)   
```

```python
def is_palindrome(input_string):
    input_string = input_string.replace(" ", " ") # 문자열 중간에 있는 공백 제거
    # length = len(input_string) #1번째 방법
    # for i in range(length//2):
    #     length - 1
    #     if input_string[i] == input_string[length - 1 - i]:
    #         #회문
    #         return False
    # return True

    return input_string == input_string[::-1] #2번째 방법(리스트를 뒤집을 수 있음.)      

result = is_palindrome("다시 합창합시다")
print(result)

reversed("안녕하세요") # 내장함수
li1 = [1,2,3,4]
# li1.reverse()
# li2.reverse(li1)
# print(li1)
# print(li2)

string1 = "안녕하세요"
string2 = reversed(string1)
for i in string2:
    print()
```

### crawling
- 데이터 수집 단계
1. 데이터 수집 → 데이터 분석/처리 → 인공지능 모델 학습 → 모델평가

-- 외부 라이브러리 --
1. pip install beautifulsoup4
2. pip install requests
   
#### crawling(html)
- http(hypertext transfer protocol)
- request - response
- client - server
- html - 웹사이트를 표현하기 위한 언어 
- < html> < /html>
- html 파싱 

```python
import requests

url = "http://www.naver.com/"
response = requests.get(url)
status = response.status_code
html = response.text
print(status)
print(html)
```

#### http 상태코드
- 200 : OK
- ~ : 요청 성공
- 302 : 리다이렉트
- ~ : 다른 페이지로 바로 연결
- 400 : Bad Request 요청이 잘못됨
- 401 : Unauthorized 비인증됨
- 402 : 
- 403 : Forbidden 접근 권한 없음
- 404 : Not Found 요청받은 내용이 없음
- 405 : Merhond Not Allowed 접근 방법이 잘못 됨
- 500 : Internal Sever Rrror 서버 에러
- 501 : Not Implemented 개발중인 서버
- 502 : Bad Gateway 잘못된 응답

#### url 구조
- 포로토콜://호스트주소:포트번호/경로?쿼리
- http://naver.com/?~~~~~
- 쿼리 - 추가적인 데이터를 포함할 때 쓰는 것.
- 쿼리이름=값&쿼리이름=값&쿼리이름=값

#### BeautifulSoup
- html 파싱(parsing)
- html을 객체 구조화해서 사용
- html 태그
- <태그이름 속성 = 송성값>내용</ 태그이름>
- from bs4 import BeautifulSoup
```python
html = "<html><body>Hello</body></html>"
soup = BeautifulSoup(html, "html.parser") # 객체가 만들어져 soup에 변수 선언
print(soup.body.text)
print(type(soup.body.text))
```
```python
equests
from bs4 import BeautifulSoup
search_url = "https://www.google.com/search?q=%EB%A7%A5%EC%A3%BC&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiKvuiS_cv-AhVT_mEKHY1qA9AQ_AUoAXoECAEQAw&biw=747&bih=959&dpr=1"
keyword = "맥주"
url = search_url + keyword
response = requests.get(url)
# print(response.text)
# print(type(response.text))
html = response.text
soup = BeautifulSoup(html, "html.parser")
imgs = soup.find_all('img')
import os #운영체제 관련된 기능을 사용가능(파일폴더 생성)
folder_name = "imgs"
if not os.path.exists(folder_name):
    os.mkdir("imgs")
for img in enumerate(imgs[1:]):
    file_name = "img.jpg"
    file_path = os.path.join(folder_name, file_name)
    img_response = requests.get(imd['src'])
    img_data = img_response.content
    with open(file_path, "wb") as f:#'wb' : 비트를 쓰겟다.
        f.write(img_data)
```

#### enumerate(이터러블)
- 번호를 붙인다.
```python
li1 = ["김연아", "류현진", "손흥민"]
for idx, name in enumerate(li1):
    print(name)
```

```python
print(soup.finf('div')[4])
print(imgs)
container = soup.finf('div', attrs = {'id' : 'container'})
print(container.find_all('img'))
```

#### 셀레니움(selenium)

```python
# 네이버 IT/과학 뉴스 크롤링

import  requests
from bs4 import BeautifulSoup
url = "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=105"
# headers = {"User-Agent" : "Mozilla/5.0"} → 크롤링 방지 회피
response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
html = response.text
soup = BeautifulSoup(html, "html.parser")
div = soup.body.find('div', attrs={'class' : 'list_body'})
headlines = div.find_all('a', attrs={'class': 'cluster_text_headline'})
import os
folder_name = "crauling_result"
if not os.path.exists("crwaling_result"):
    os.mkdir(folder_name)
for headline in headlines:
    # 과제 : crawling_result 폴더의 headlines.txt 파일에 저장하기.
    # with open("crawling_result/headlines.txt", "a", encoding="utf-8") as f:
    # f.write(headline.text.strip())
    # f.write("\n")
    # print(html.text.strip())
    article_response = requests.get(headline['href'])
    aritcle_soup = BeautifulSoup(article_response.text, "html.parser")
    article = aritcle_soup.find('div', attrs={"id": "dic_area"})
    print(article.text)
```
